{\rtf1\ansi\ansicpg1252\cocoartf2509
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red212\green34\blue255;\red0\green0\blue0;\red224\green51\blue34;
\red178\green48\blue128;\red97\green34\blue255;\red209\green113\blue37;\red119\green117\blue165;\red56\green159\blue157;
}
{\*\expandedcolortbl;;\cssrgb\c87269\c29650\c100000;\csgray\c0;\cssrgb\c91311\c29126\c17209;
\cssrgb\c75800\c28661\c57528;\cssrgb\c46228\c27439\c100000;\cssrgb\c85991\c52029\c18734;\cssrgb\c54197\c53905\c70624;\cssrgb\c26122\c67994\c67783;
}
\margl1440\margr1440\vieww14200\viewh16580\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs22 \cf2 \CocoaLigature0 from\cf3  numpy \cf2 import\cf3  array \cf4 #                                                                            \cf3 \
\cf2 import\cf3  numpy \cf2 as\cf3  np\
\cf2 from\cf3  pickle \cf2 import\cf3  load \cf4 #                                                                            \cf3 \
\cf2 from\cf3  keras.preprocessing.text \cf2 import\cf3  Tokenizer \cf4 #                                                     \cf3 \
\cf2 from\cf3  keras.preprocessing.sequence \cf2 import\cf3  pad_sequences \cf4 #                                             \cf3 \
\cf2 from\cf3  keras.utils \cf2 import\cf3  to_categorical \cf4 #                                                             \cf3 \
\cf2 from\cf3  keras.utils \cf2 import\cf3  plot_model \cf4 #                                                                 \cf3 \
\cf2 from\cf3  keras.models \cf2 import\cf3  Model \cf4 #                                                                     \cf3 \
\cf2 from\cf3  keras.layers \cf2 import\cf3  Input \cf4 #                                                                     \cf3 \
\cf2 from\cf3  keras.layers \cf2 import\cf3  Dense \cf4 #                                                                     \cf3 \
\cf2 from\cf3  keras.layers \cf2 import\cf3  LSTM \cf4 #                                                                      \cf3 \
\cf2 from\cf3  keras.layers \cf2 import\cf3  Embedding \cf4 #                                                                 \cf3 \
\cf2 from\cf3  keras.layers \cf2 import\cf3  Dropout \cf4 #                                                                   \cf3 \
\cf2 from\cf3  keras.layers.merge \cf2 import\cf3  add \cf4 #                                                                 \cf3 \
\cf2 from\cf3  keras.callbacks \cf2 import\cf3  ModelCheckpoint \cf4 #                                                        \cf3 \
\cf5 '''                                                                                                  \cf3 \
\cf5 Load image train folder with txt files and store the name of the images                              \cf3 \
\cf5 '''\cf3 \
\cf2 def\cf3  \cf6 load_set\cf3 (filename): \cf4 #.                                                                           \cf3 \
    \cf7 doc\cf3  = load_doc(filename)\
    \cf7 dataset\cf3  = \cf8 list\cf3 ()\
    \cf4 # process line by line                                                                           \cf3 \
    \cf2 for\cf3  token \cf2 in\cf3  doc.split():\
        \cf7 token\cf3  = token.strip()\
        \cf4 # skip empty lines                                                                           \cf3 \
        \cf2 if\cf3  token == \cf5 ''\cf3 :\
            \cf2 continue\cf3 \
        \cf7 identifier\cf3  = token.split(\cf5 '.'\cf3 )[0]\
        dataset.append(identifier)\
    \cf2 return\cf3  \cf8 set\cf3 (dataset)\
\
\cf5 '''                                                                                                  \cf3 \
\cf5 load the document into memory                                                                        \cf3 \
\cf5 '''\cf3 \
\cf2 def\cf3  \cf6 load_doc\cf3 (filename): \cf4 #                                                                            \cf3 \
    \cf8 file\cf3  = \cf8 open\cf3 (filename, \cf5 'r'\cf3 )\
    \cf4 # read all text                                                                                  \cf3 \
    \cf7 text\cf3  = \cf8 file\cf3 .read()\
    \cf8 file\cf3 .close()\
    \cf2 return\cf3  text\
\
\cf5 '''                                                                                                  \cf3 \
\cf5  load clean descriptions into memory                                                                 \cf3 \
\cf5 '''\cf3 \
\cf2 def\cf3  \cf6 load_clean_descriptions\cf3 (filename, dataset):  \cf4 #.                                                  \cf3 \
    \cf2 print\cf3 (\cf5 "size of dataset:"\cf3 , \cf8 len\cf3 (dataset))\
    \cf4 # load document                                                                                  \cf3 \
    \cf7 doc\cf3  = load_doc(filename)\
    \cf7 descriptions\cf3  = \cf8 dict\cf3 ()\
    \cf2 print\cf3 (\cf5 "number of lines in document:"\cf3 , \cf8 len\cf3 (doc.split(\cf5 '\\n'\cf3 )))\
    \cf7 images\cf3  = []\
    \cf2 for\cf3  line \cf2 in\cf3  doc.split(\cf5 '\\n'\cf3 ):\
        \cf4 # split line by white space                                                                  \cf3 \
        \cf7 tokens\cf3  = line.split()\
        \cf4 # split id from description                                                                  \cf3 \
        \cf2 if\cf3  \cf8 len\cf3 (tokens) < 2:\
            \cf2 continue\cf3 \
       images.append(tokens[0])\
        \cf7 image_id\cf3 , \cf7 image_desc\cf3  = tokens[0], tokens[1:]\
        \cf4 # skip images not in the set                                                                 \cf3 \
        \cf2 if\cf3  image_id \cf2 in\cf3  dataset:\
            \cf4 # create list                                                                            \cf3 \
            \cf2 if\cf3  image_id \cf2 not\cf3  \cf2 in\cf3  descriptions:\
                \cf7 descriptions\cf3 [image_id] = \cf8 list\cf3 ()\
            \cf4 # wrap descriptions in tokens                                                            \cf3 \
            \cf7 desc\cf3  = \cf5 'startseq '\cf3  + \cf5 ' '\cf3 .join(image_desc) + \cf5 ' endseq'\cf3 \
            \cf4 # store                                                                                  \cf3 \
            descriptions[image_id].append(desc)\
    \cf2 print\cf3 (\cf5 "number of descriptions found:"\cf3 , \cf8 len\cf3 (descriptions))\
    \cf2 print\cf3 (\cf5 "number of unique captioned images:"\cf3 , \cf8 len\cf3 (np.unique(images)))\
    \cf2 return\cf3  descriptions\
\
\
\cf4 # load photo features                                                                                \cf3 \
\cf2 def\cf3  \cf6 load_photo_features\cf3 (filename, dataset): \cf4 #                                                        \cf3 \
    \cf4 # load all features                                                                              \cf3 \
    \cf7 all_features\cf3  = load(\cf8 open\cf3 (filename, \cf5 'rb'\cf3 ))\
    \cf4 # filter features                                                                                \cf3 \
    \cf7 features\cf3  = \{k: all_features[k] \cf2 for\cf3  k \cf2 in\cf3  dataset\}\
    \cf2 return\cf3  features\
\
\cf4 # fit a tokenizer given caption descriptions                                                         \cf3 \
\cf2 def\cf3  \cf6 create_tokenizer\cf3 (descriptions): \cf4 #                                                                \cf3 \
    \cf7 lines\cf3  = to_lines(descriptions)\
    \cf7 tokenizer\cf3  = Tokenizer()\
    tokenizer.fit_on_texts(lines)\
    \cf2 return\cf3  tokenizer\
\
\cf4 # convert a dict of clean descriptions to a list of descriptions                                     \cf3 \
\cf2 def\cf3  \cf6 to_lines\cf3 (descriptions): \cf4 #                                                                        \cf3 \
    \cf7 all_desc\cf3  = \cf8 list\cf3 ()\
    \cf2 print\cf3 (\cf5 "number of image ids in descriptions:"\cf3 , \cf8 len\cf3 (descriptions.keys()))\
    \cf2 for\cf3  key \cf2 in\cf3  descriptions.keys():\
        [all_desc.append(d) \cf2 for\cf3  d \cf2 in\cf3  descriptions[key]]\
    \cf2 return\cf3  all_desc\
\
\cf4 # calculate the length of the description with the most words                                        \cf3 \
\cf2 def\cf3  \cf6 max_length\cf3 (descriptions): \cf4 #                                                                      \cf3 \
    \cf7 lines\cf3  = to_lines(descriptions)\
    \cf2 print\cf3 (\cf5 "number of annotation lines:"\cf3 , \cf8 len\cf3 (lines))\
    \cf2 print\cf3 (\cf5 "example:"\cf3 , lines[0].split())\
    \cf2 return\cf3  \cf8 max\cf3 (\cf8 len\cf3 (d.split()) \cf2 for\cf3  d \cf2 in\cf3  lines)\
\
\cf4 # create sequences of images, input sequences and output words for an image                          \cf3 \
\cf2 def\cf3  \cf6 create_sequences\cf3 (tokenizer, max_length, desc_list, photo, vocab_size): \cf4 #                         \cf3 \
    \cf7 X1\cf3 , \cf7 X2\cf3 , \cf7 y\cf3  = \cf8 list\cf3 (), \cf8 list\cf3 (), \cf8 list\cf3 ()\
    \cf4 # walk through each description for the image                                                    \cf3 \
    \cf2 for\cf3  desc \cf2 in\cf3  desc_list:\
    \cf4 # encode the sequence                                                                            \cf3 \
        \cf7 seq\cf3  = tokenizer.texts_to_sequences([desc])[0]\
        \cf4 # split one sequence into multiple X,y pairs                                                 \cf3 \
        \cf2 for\cf3  i \cf2 in\cf3  \cf8 range\cf3 (1, \cf8 len\cf3 (seq)):\
        \cf4 # split into input and output pair                                                           \cf3 \
            \cf7 in_seq\cf3 , \cf7 out_seq\cf3  = seq[:i], seq[i]\
            \cf4 # pad input sequence                                                                     \cf3 \
            \cf7 in_seq\cf3  = pad_sequences([in_seq], maxlen=max_length)[0]\
            \cf4 # encode output sequence                                                                 \cf3 \
            \cf7 out_seq\cf3  = to_categorical([out_seq], num_classes=vocab_size)[0]\
            \cf4 # store                                                                                  \cf3 \
            X1.append(photo)\
            X2.append(in_seq)\
            y.append(out_seq)\
    \cf2 return\cf3  array(X1), array(X2), array(y)\
\
\cf4 # define the captioning model                                                                        \cf3 \
\cf2 def\cf3  \cf6 define_model\cf3 (vocab_size, max_length): \cf4 #                                                          \cf3 \
    \cf4 # feature extractor model                                                                        \cf3 \
    \cf7 inputs1\cf3  = Input(shape=(4096,))\
    \cf7 fe1\cf3  = Dropout(0.5)(inputs1)\
    \cf7 fe2\cf3  = Dense(256, activation=\cf5 'relu'\cf3 )(fe1)\
    \cf4 # sequence model                                                                                 \cf3 \
    \cf7 inputs2\cf3  = Input(shape=(max_length,))\
    \cf7 se1\cf3  = Embedding(vocab_size, 256, mask_zero=\cf9 True\cf3 )(inputs2)\
    \cf7 se2\cf3  = Dropout(0.5)(se1)\
    \cf7 se3\cf3  = LSTM(256)(se2)\
    \cf4 # decoder model                                                                                  \cf3 \
    \cf7 decoder1\cf3  = add([fe2, se3])\
    \cf7 decoder2\cf3  = Dense(256, activation=\cf5 'relu'\cf3 )(decoder1)\
    \cf7 outputs\cf3  = Dense(vocab_size, activation=\cf5 'softmax'\cf3 )(decoder2)\
    \cf4 # tie it together [image, seq] [word]                                                            \cf3 \
    \cf7 model\cf3  = Model(inputs=[inputs1, inputs2], outputs=outputs)\
    \cf4 # compile model                                                                                  \cf3 \
    model.\cf8 compile\cf3 (loss=\cf5 'categorical_crossentropy'\cf3 , optimizer=\cf5 'adam'\cf3 )\
    \cf4 # summarize model                                                                                \cf3 \
    model.summary()\
    plot_model(model, to_file=\cf5 'model.png'\cf3 , show_shapes=\cf9 True\cf3 )\
    \cf2 return\cf3  model\
\
\cf4 # data generator, intended to be used in a call to model.fit_generator()                             \cf3 \
\cf2 def\cf3  \cf6 data_generator\cf3 (descriptions, photos, tokenizer, max_length, vocab_size): \cf4 #                       \cf3 \
    \cf4 # loop for ever over images                                                                      \cf3 \
    \cf2 while\cf3  1:\
        \cf2 for\cf3  key, desc_list \cf2 in\cf3  descriptions.items():\
            \cf4 # retrieve the photo feature                                                             \cf3 \
            \cf7 photo\cf3  = photos[key][0]\
            \cf7 in_img\cf3 , \cf7 in_seq\cf3 , \cf7 out_word\cf3  = create_sequences(tokenizer, max_length, desc_list, photo, voc\\\
ab_size)\
            \cf2 yield\cf3  [[in_img, in_seq], out_word]\
\
\
\cf4 # load training dataset                                                                             \cf3 \
\cf7 filename\cf3  = \cf5 "Train2014IDs.txt"\cf3  \cf4 #                                                                      \cf3 \
\cf7 train\cf3  = load_set(filename) \cf4 #                                                                         \cf3 \
\cf2 print\cf3 (\cf5 "Dataset total train image count: %d"\cf3  % \cf8 len\cf3 (train))\
\
\cf4 # descriptions                                                                                      \cf3 \
\cf7 train_descriptions\cf3  = load_clean_descriptions(\cf5 'trainDescriptions.txt'\cf3 , train) \cf4 #                       \cf3 \
\cf2 print\cf3 (\cf5 "Descriptions: train=%d"\cf3   % \cf8 len\cf3 (train_descriptions)) \cf4 #                                         \cf3 \
\
\cf4 # photo features                                                                                     \cf3 \
\cf7 train_features\cf3  = load_photo_features(\cf5 'features.pkl'\cf3 , train) \cf4 #                                        \cf3 \
\cf2 print\cf3 (\cf5 'Photos: train=%d'\cf3  % \cf8 len\cf3 (train_features)) \cf4 #                                                    \cf3 \
\
\cf4 # prepare tokenizer                                                                                  \cf3 \
\cf7 tokenizer\cf3  = create_tokenizer(train_descriptions) \cf4 #                                                   \cf3 \
\cf7 vocab_size\cf3  = \cf8 len\cf3 (tokenizer.word_index) + 1 \cf4 #                                                         \cf3 \
\cf2 print\cf3 (\cf5 'Vocab Size: %d'\cf3  % vocab_size) \cf4 #                                                               \cf3 \
\
\cf4 # Determine the maximum sequence length                                                              \cf3 \
\cf7 max_length\cf3  = max_length(train_descriptions) \cf4 #                                                        \cf3 \
\cf2 print\cf3 (\cf5 "Description Length: %d"\cf3  % max_length) \cf4 #                                                       \cf3 \
\
\cf4 # Define the model                                                                                   \cf3 \
\cf7 model\cf3  = define_model(vocab_size, max_length) \cf4 #                                                       \cf3 \
\cf4 # train the model, run epochs manually and save after each epoch                                     \cf3 \
\cf7 epochs\cf3  = 20 \cf4 #                                                                                        \cf3 \
\cf7 steps\cf3  = \cf8 len\cf3 (train_descriptions) \cf4 #                                                                    \cf3 \
\
\cf4 # Creating models                                                                                    \cf3 \
\cf5 '''                                                                                                  \cf3 \
\cf5 for i in range(epochs):                                                                              \cf3 \
\cf5     # create the data generator                                                                      \cf3 \
\cf5     generator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size\cf3 \\\
\cf5 )                                                                                                    \cf3 \
\cf5     # fit for one epoch                                                                              \cf3 \
\cf5     model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)                       \cf3 \
\cf5     # save model                                                                                     \cf3 \
\cf5     model.save('model_' + str(i) + '.h5')                                                            \cf3 \
\cf5 '''\cf3 \
\cf2 from\cf3  numpy \cf2 import\cf3  argmax \cf4 #                                                                           \cf3 \
\cf2 from\cf3  keras.models \cf2 import\cf3  load_model \cf4 #                                                                \cf3 \
\cf2 from\cf3  nltk.translate.bleu_score \cf2 import\cf3  corpus_bleu \cf4 #                                                  \cf3 \
\
\cf4 # calculate the length of the description with the most words                                        \cf3 \
\cf2 def\cf3  \cf6 max_length\cf3 (descriptions): \cf4 #                                                                      \cf3 \
        \cf7 lines\cf3  = to_lines(descriptions)\
        \cf2 return\cf3  \cf8 max\cf3 (\cf8 len\cf3 (d.split()) \cf2 for\cf3  d \cf2 in\cf3  lines)\
\
\
\cf4 # map an integer to a word                                                                           \cf3 \
\cf2 def\cf3  \cf6 word_for_id\cf3 (integer, tokenizer): \cf4 #                                                               \cf3 \
        \cf2 for\cf3  word, index \cf2 in\cf3  tokenizer.word_index.items():\
                \cf2 if\cf3  index == integer:\
                        \cf2 return\cf3  word\
        \cf2 return\cf3  \cf9 None\cf3 \
\
\cf4 # generate a description for an image                                                                \cf3 \
\cf2 def\cf3  \cf6 generate_desc\cf3 (model, tokenizer, photo, max_length): \cf4 #                                            \cf3 \
        \cf4 # seed the generation process                                                                \cf3 \
        \cf7 in_text\cf3  = \cf5 'startseq'\cf3 \
        \cf4 # iterate over the whole length of the sequence                                              \cf3 \
        \cf2 for\cf3  i \cf2 in\cf3  \cf8 range\cf3 (max_length):\
                \cf4 # integer encode input sequence                                                      \cf3 \
                \cf7 sequence\cf3  = tokenizer.texts_to_sequences([in_text])[0]\
                \cf4 # pad input                                                                          \cf3 \
                \cf7 sequence\cf3  = pad_sequences([sequence], maxlen=max_length)\
                \cf4 # predict next word                                                                  \cf3 \
                \cf7 yhat\cf3  = model.predict([photo,sequence], verbose=0)\
                \cf4 # convert probability to integer                                                     \cf3 \
                \cf7 yhat\cf3  = argmax(yhat)\
                \cf4 # map integer to word                                                                \cf3 \
                \cf7 word\cf3  = word_for_id(yhat, tokenizer)\
                \cf4 # stop if we cannot map the word                                                     \cf3 \
                \cf2 if\cf3  word \cf2 is\cf3  \cf9 None\cf3 :\
                        \cf2 break\cf3 \
                \cf4 # append as input for generating the next word                                       \cf3 \
                \cf7 in_text\cf3  += \cf5 ' '\cf3  + word\
                \cf4 # stop if we predict the end of the sequence                                         \cf3 \
                \cf2 if\cf3  word == \cf5 'endseq'\cf3 :\
                        \cf2 break\cf3 \
        \cf2 return\cf3  in_text\
\
\cf4 # evaluate the skill of the model                                                                    \cf3 \
\cf2 def\cf3  \cf6 evaluate_model\cf3 (model, descriptions, photos, tokenizer, max_length): \cf4 #                            \cf3 \
        \cf7 actual\cf3 , \cf7 predicted\cf3  = \cf8 list\cf3 (), \cf8 list\cf3 ()\
        \cf4 # step over the whole set                                                                    \cf3 \
        \cf2 for\cf3  key, desc_list \cf2 in\cf3  descriptions.items():\
                \cf4 # generate description                                                               \cf3 \
                \cf7 yhat\cf3  = generate_desc(model, tokenizer, photos[key], max_length)\
                \cf4 # store actual and predicted                                                         \cf3 \
                \cf7 references\cf3  = [d.split() \cf2 for\cf3  d \cf2 in\cf3  desc_list]\
                actual.append(references)\
                predicted.append(yhat.split())\
        \cf4 # calculate BLEU score                                                                       \cf3 \
        \cf2 print\cf3 (\cf5 'BLEU-1: %f'\cf3  % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\
        \cf2 print\cf3 (\cf5 'BLEU-2: %f'\cf3  % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\
        \cf2 print\cf3 (\cf5 'BLEU-3: %f'\cf3  % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\
        \cf2 print\cf3 (\cf5 'BLEU-4: %f'\cf3  % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\
\
\cf4 # load training dataset (6K)                                                                         \cf3 \
\cf7 filename\cf3  = \cf5 'Train2014IDs.txt'\cf3  \cf4 #                                                                      \cf3 \
\cf7 train\cf3  = load_set(filename)  \cf4 #                                                                        \cf3 \
\cf2 print\cf3 (\cf5 'Dataset: %d'\cf3  % \cf8 len\cf3 (train)) \cf4 #                                                                  \cf3 \
\cf4 # descriptions                                                                                       \cf3 \
\cf7 train_descriptions\cf3  = load_clean_descriptions(\cf5 'trainDescriptions.txt'\cf3 , train) \cf4 #                       \cf3 \
\cf2 print\cf3 (\cf5 'Descriptions: train=%d'\cf3  % \cf8 len\cf3 (train_descriptions))\
\cf4 # prepare tokenizer                                                                                  \cf3 \
\cf7 tokenizer\cf3  = create_tokenizer(train_descriptions) \cf4 #                                                   \cf3 \
\cf7 vocab_size\cf3  = \cf8 len\cf3 (tokenizer.word_index) + 1 \cf4 #                                                         \cf3 \
\cf2 print\cf3 (\cf5 'Vocabulary Size: %d'\cf3  % vocab_size) \cf4 #                                                          \cf3 \
\cf4 # determine the maximum sequence length                                                              \cf3 \
\cf7 max_length\cf3  = max_length(train_descriptions) \cf4 #                                                        \cf3 \
\cf2 print\cf3 (\cf5 'Description Length: %d'\cf3  % max_length) \cf4 #                                                       \cf3 \
\
\cf7 filename\cf3  = \cf5 'val2014.txt'\cf3  \cf4 #                                                                           \cf3 \
\cf7 test\cf3  = load_set(filename) \cf4 #                                                                          \cf3 \
\cf7 test_descriptions\cf3  = load_clean_descriptions(\cf5 'valDescriptions.txt'\cf3 , test) \cf4 #                           \cf3 \
\
\cf4 # load the model                                                                                     \cf3 \
\cf7 filename\cf3  = \cf5 'model_12.h5'\cf3  \cf4 #                                                                           \cf3 \
\cf7 model\cf3  = load_model(filename) \cf4 #          \cf3 \
\
\cf2 print\cf3 (\cf5 'Descriptions: test=%d'\cf3  % \cf8 len\cf3 (test_descriptions)) \cf4 #                                            \cf3 \
\cf4 # photo features                                                                                     \cf3 \
\cf7 test_features\cf3  = load_photo_features(\cf5 'valfeatures.pkl'\cf3 , test) \cf4 #                                       \cf3 \
\cf2 print\cf3 (\cf5 'Photos: test=%d'\cf3  % \cf8 len\cf3 (test_features)) \cf4 #                                                      \cf3 \
\cf4 # evaluate model                                                                                     \cf3 \
evaluate_model(model, test_descriptions, test_features, tokenizer, max_length) \cf4 #                     \cf3 \
\
\cf2 from\cf3  pickle \cf2 import\cf3  dump \cf4 #                                                                            \cf3 \
\cf4 # load training dataset (6K)                                                                         \cf3 \
\cf7 filename\cf3  = \cf5 'Train2014IDs.txt'\cf3  \cf4 #                                                                      \cf3 \
\cf7 train\cf3  = load_set(filename) \cf4 #                                                                         \cf3 \
\cf2 print\cf3 (\cf5 'Dataset: %d'\cf3  % \cf8 len\cf3 (train)) \cf4 #                                                                  \cf3 \
\cf4 # descriptions                                                                                       \cf3 \
\cf7 train_descriptions\cf3  = load_clean_descriptions(\cf5 'trainDescriptions.txt'\cf3 , train) \cf4 #                       \cf3 \
\cf2 print\cf3 (\cf5 'Descriptions: train=%d'\cf3  % \cf8 len\cf3 (train_descriptions)) \cf4 #                                          \cf3 \
\
\cf4 # prepare tokenizer                                                                                  \cf3 \
\cf7 tokenizer\cf3  = create_tokenizer(train_descriptions) \cf4 #                                                   \cf3 \
\cf4 # save the tokenizer                                                                                 \cf3 \
dump(tokenizer, \cf8 open\cf3 (\cf5 'tokenizer.pkl'\cf3 , \cf5 'wb'\cf3 )) \cf4 #                                                       \cf3 \
\
\cf2 from\cf3  keras.applications.vgg16 \cf2 import\cf3  VGG16 \cf4 #                                                         \cf3 \
\cf2 from\cf3  keras.applications.vgg16 \cf2 import\cf3  preprocess_input \cf4 #                                              \cf3 \
\cf2 from\cf3  keras.preprocessing.image \cf2 import\cf3  load_img \cf4 #                                                     \cf3 \
\cf2 from\cf3  keras.preprocessing.image \cf2 import\cf3  img_to_array \cf4 #                                                 \cf3 \
\
\cf4 # extract features from each photo in the directory                                                  \cf3 \
\cf2 def\cf3  \cf6 extract_features\cf3 (filename): \cf4 #                                                                    \cf3 \
    \cf4 # load the model                                                                                 \cf3 \
    \cf7 model\cf3  = VGG16()\
    \cf4 # re-structure the model                                                                         \cf3 \
    model.layers.pop()\
    \cf7 model\cf3  = Model(inputs=model.inputs, outputs=model.layers[-1].output)\
    \cf4 # load the photo                                                                                 \cf3 \
    \cf7 image\cf3  = load_img(filename, target_size=(224, 224))\
    \cf4 # convert the image pixels to a numpy array                                                      \cf3 \
    \cf7 image\cf3  = img_to_array(image)\
    \cf4 # reshape data for the model                                                                     \cf3 \
    \cf7 image\cf3  = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\
    \cf4 # prepare the image for the VGG model                                                            \cf3 \
    \cf7 image\cf3  = preprocess_input(image)\
    \cf4 # get features                                                                                   \cf3 \
    \cf7 feature\cf3  = model.predict(image, verbose=0)\
    \cf2 return\cf3  feature\
\
\cf4 # load the tokenizer                                                                                 \cf3 \
\cf7 tokenizer\cf3  = load(\cf8 open\cf3 (\cf5 'tokenizer.pkl'\cf3 , \cf5 'rb'\cf3 )) \cf4 #                                                      \cf3 \
\cf4 # pre-define the max sequence length (from training)                                                 \cf3 \
\cf7 max_length\cf3  = 51 \cf4 #.                                                                                   \cf3 \
\cf4 # load the model                                                                                     \cf3 \
\cf7 model\cf3  = load_model(\cf5 'model_12.h5'\cf3 ) \cf4 #                                                                  \cf3 \
\cf4 # load and prepare the photograph                                                                    \cf3 \
\cf7 photo\cf3  = extract_features(\cf5 'bus.jpg'\cf3 ) \cf4 #                                                                \cf3 \
\cf4 # generate description                                                                               \cf3 \
\cf7 description\cf3  = generate_desc(model, tokenizer, photo, max_length) \cf4 #                                   \cf3 \
\cf2 print\cf3 (description) \cf4 #                      \cf3 \
\
\
\
\
\
\
\
\
\
}